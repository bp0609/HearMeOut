{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1503285b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devil/Documents/Courses/HCI/Model/.venv/lib/python3.13/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.output.bias', 'classifier.output.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.output.bias', 'classifier.output.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, AutoModelForAudioClassification\n",
    "\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d364737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict emotions from audio inputs\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "def predict_emotion(audio_path, sampling_rate=16000, max_duration=30):\n",
    "    \"\"\"\n",
    "    Predict emotion from an audio file\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to the audio file\n",
    "        sampling_rate: Target sampling rate (default: 16000 Hz)\n",
    "        max_duration: Maximum duration in seconds to process (default: 30)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with emotion predictions and scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get audio info first to check validity\n",
    "        info = sf.info(audio_path)\n",
    "        print(f\"Audio info: {info.duration:.2f}s, {info.samplerate}Hz, {info.channels} channel(s)\")\n",
    "        \n",
    "        # Load audio file with duration limit\n",
    "        duration = min(info.duration, max_duration) if max_duration else None\n",
    "        audio, sr = librosa.load(audio_path, sr=sampling_rate, duration=duration)\n",
    "        \n",
    "        # Ensure audio is not empty\n",
    "        if len(audio) == 0:\n",
    "            raise ValueError(\"Audio file is empty or could not be loaded\")\n",
    "        \n",
    "        print(f\"Loaded audio: {len(audio)} samples, {len(audio)/sampling_rate:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading audio file: {str(e)}\")\n",
    "    \n",
    "    # Process audio with feature extractor\n",
    "    inputs = processor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    # Get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get predicted class and confidence\n",
    "    predicted_class_id = torch.argmax(probabilities, dim=-1).item()\n",
    "    confidence = probabilities[0][predicted_class_id].item()\n",
    "    \n",
    "    # Get emotion label\n",
    "    predicted_emotion = model.config.id2label[predicted_class_id]\n",
    "    \n",
    "    # Get all emotion scores\n",
    "    all_emotions = {model.config.id2label[i]: probabilities[0][i].item() \n",
    "                    for i in range(len(model.config.id2label))}\n",
    "    \n",
    "    return {\n",
    "        'emotion': predicted_emotion,\n",
    "        'confidence': confidence,\n",
    "        'all_scores': all_emotions\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# result = predict_emotion('path/to/your/audio.wav')\n",
    "# print(f\"Predicted emotion: {result['emotion']}\")\n",
    "# print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "# print(f\"\\nAll emotion scores:\")\n",
    "# for emotion, score in result['all_scores'].items():\n",
    "#     print(f\"  {emotion}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386fb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio info: 5.12s, 16000Hz, 1 channel(s)\n",
      "Loaded audio: 81920 samples, 5.12s\n",
      "Predicted emotion: surprised\n",
      "Confidence: 13.10%\n",
      "\n",
      "All emotion scores:\n",
      "  surprised: 13.10%\n",
      "  happy: 12.89%\n",
      "  sad: 12.59%\n",
      "  angry: 12.48%\n",
      "  disgust: 12.36%\n",
      "  fearful: 12.28%\n",
      "  neutral: 12.16%\n",
      "  calm: 12.14%\n"
     ]
    }
   ],
   "source": [
    "# Test with an audio file\n",
    "# Replace with the path to your MP3 file\n",
    "audio_file = 'your_audio.mp3'  # Change this to your MP3 file path\n",
    "\n",
    "# Predict emotion from the audio file\n",
    "result = predict_emotion(audio_file)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Predicted emotion: {result['emotion']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"\\nAll emotion scores (sorted by confidence):\")\n",
    "for emotion, score in sorted(result['all_scores'].items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {emotion:12s}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "551546de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.1.1-1ubuntu1.2 Copyright (c) 2000-2025 the FFmpeg developers\n",
      "  built with gcc 14 (Ubuntu 14.2.0-19ubuntu2)\n",
      "  configuration: --prefix=/usr --extra-version=1ubuntu1.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-libmfx --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libcaca --enable-libdvdnav --enable-libdvdread --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.101 / 61. 19.101\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "Input #0, flac, from 'temp3.wav':\n",
      "  Duration: N/A, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Audio: flac, 44100 Hz, stereo, s16\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (flac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'temp.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf61.7.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
      "      Metadata:\n",
      "        encoder         : Lavc61.19.101 pcm_s16le\n",
      "\u001b[1;35m[out#0/wav @ 0x654b7634e180] \u001b[0mvideo:0KiB audio:160KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.047607%\n",
      "size=     160KiB time=00:00:05.12 bitrate= 256.1kbits/s speed= 360x    \n",
      "\u001b[1;35m[out#0/wav @ 0x654b7634e180] \u001b[0mvideo:0KiB audio:160KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.047607%\n",
      "size=     160KiB time=00:00:05.12 bitrate= 256.1kbits/s speed= 360x    \n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -i temp3.wav -ar 16000 -ac 1 temp.wav -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55730727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
